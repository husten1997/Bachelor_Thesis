---
title: "Overview"
author: "Julian Dietlmeier"
date: "March 29, 2019"
output:
  html_document:
    fig_height: 5
    fig_width: 7
    highlight: pygments
    theme: cerulean
  word_document: default
---

Das ist ein kurzer Überblick über meine Bachelorarbeit mit Code und Ergebnissen. Ziel ist es, am Beispiel Microsoft ein mal durch all meine Berechnungen zu führen. Begonnen wird mit dem laden der benötigten Paketen und meiner eigenen Funktionen (aus functions.R, dienen überwiegend der Vereinfachung von Import und plotten von Verteilungen oder Markierung von extrem Werten, die Funktionen werden später vorgestellt). Kommentare im Code sind häufig alternative Methodiken.  
  
## 0) Set Up  
```{r Set_Up, message=FALSE, warning=FALSE}
rm(list = ls())
library(readxl)
library(xts)
library(forecast)
library(tseries)

source("functions.R")
```
  
## 1) Import 
  
Import der Fundamental- (BV) und Preisdaten (MV). Diese werden in einer eigenen Enviroment gespeichert. Hiermit wird versucht die Daten von verschiedenen Firmen abzugrenzen. Außerdem macht es den Import mittels einer Funktion überhaupt erst möglich.
  
```{r MSFT Import}
P_start_date <- "1986-03-13"
BV_start_date <- "/1990-03-29"
count_of_days <- 12017

MSFT <- new.env()
MSFT$BV <- read_excel("Data_Eikon/American_Electronics/Microsoft.xlsx", 
                      sheet = "Tabelle1", col_types = c("date", 
                                                        "numeric", "numeric", "numeric", 
                                                        "numeric", "numeric", "numeric", "numeric", "numeric"))
MSFT$MV <- read_excel("Data_Eikon/American_Electronics/Microsoft.xlsx", 
                      sheet = "Prices", col_types = c("date", 
                                                      "numeric"))
```

  
## 2) Vorbereitung 
  
Nachdem die Daten importiert wurden, wird eine weitere wichtige Kennzahl berechnet. Der "twelve months trailing Earnings per Share":
$$ EPSttm_{t} = \frac{\sum_{i = 0}^{3} Earn_{t-i}}{Shares_{t}} $$
Mithilfe des EPS ttm, werden negative Gewinne in einem Quartal durch evtl positive Gewinne in einem anderen Quartal des selben Jahres ausgegelichen. Das führt zu weniger verlorenen Datenpunkten, da man Multiples auf Basis von negativen Wertetreibern nicht interpreteiren und somit nicht in die Analyse eingebziehen kann.
  
Im Anschluss werden noch die Änderungen des BPS und des EPS berechnet, dazu aber später mehr.

```{r MSFT Import 2}
MSFT$BV$EPS_ttm <- rep(0)
for(i in c((nrow(MSFT$BV)-3):1)){
  MSFT$BV$EPS_ttm[i] <- sum(MSFT$BV$NI[c((i+3):i)]) / MSFT$BV$Shares[i] 
}

#calculation of changes
MSFT$BV$d.BPS_E <- c((ts(MSFT$BV$BPS_E) - lag(ts(MSFT$BV$BPS_E)))/lag(ts(MSFT$BV$BPS_E)),0)
MSFT$BV$d.EPS_ttm <- c((ts(MSFT$BV$EPS_ttm) - lag(ts(MSFT$BV$EPS_ttm)))/lag(ts(MSFT$BV$EPS_ttm)),0)
```
  
Im Anschluss werden die Preis- und Fundamentaldaten auf eine einheitliche Zeitskala abgebildet. Dazu wird ein xts-Objekt erstellt, welches alle Tage von Anfang der Aufzeichnungen bis zum 4.2.2019 enthält, die Daten werden dann an der entsprechenden Stelle zugeordnet. 
BPS = Book Value per Share:

$$ BPS_{t} =  \frac{(Book Value of Equity)_{t}}{Shares_{t}} $$
BPS_E ist dabei auf das Equity bezogen, BPS_D auf das Debt Capital und BPS_T auf den kompletten Firmen Wert.
  
EPS = Earnings per Share:
$$ EPS_{t} = \frac{(Net Income After Tax)_{t}}{Shares_{t}}$$
  
```{r MSFT Import 3}
MSFT$Data <- data.frame(matrix(0, nrow = count_of_days, ncol = 13))
names(MSFT$Data) <- c("Date", "PPS", "BV_E", "BV_T", "Shares", "BPS_E", "BPS_D", "BPS_T", "NI", "EPS", "EPS_ttm", "d.BPS_E", "d.EPS_ttm")
MSFT$Data[,1] <- seq(as.Date(P_start_date),length=count_of_days ,by="days")



MSFT$Data[as.character.Date(MSFT$Data[,1]) %in% as.character.Date(MSFT$MV$Date) ,2] <- 
  as.numeric(rev(MSFT$MV$Close))
MSFT$Data[as.character.Date(MSFT$Data[,1]) %in% as.character.Date(MSFT$BV$Date) ,3] <- 
  as.numeric(rev(MSFT$BV$BV_E))
MSFT$Data[as.character.Date(MSFT$Data[,1]) %in% as.character.Date(MSFT$BV$Date) ,4] <- 
  as.numeric(rev(MSFT$BV$BV_T))
MSFT$Data[as.character.Date(MSFT$Data[,1]) %in% as.character.Date(MSFT$BV$Date) ,5] <- 
  as.numeric(rev(MSFT$BV$Shares))
MSFT$Data[as.character.Date(MSFT$Data[,1]) %in% as.character.Date(MSFT$BV$Date) ,6] <- 
  as.numeric(rev(MSFT$BV$BPS_E))
MSFT$Data[as.character.Date(MSFT$Data[,1]) %in% as.character.Date(MSFT$BV$Date) ,7] <- 
  as.numeric(rev(MSFT$BV$BPS_D))
MSFT$Data[as.character.Date(MSFT$Data[,1]) %in% as.character.Date(MSFT$BV$Date) ,8] <- 
  as.numeric(rev(MSFT$BV$BPS_T))
MSFT$Data[as.character.Date(MSFT$Data[,1]) %in% as.character.Date(MSFT$BV$Date) ,9] <- 
  as.numeric(rev(MSFT$BV$NI))
MSFT$Data[as.character.Date(MSFT$Data[,1]) %in% as.character.Date(MSFT$BV$Date) ,10] <- 
  as.numeric(rev(MSFT$BV$EPS))
MSFT$Data[as.character.Date(MSFT$Data[,1]) %in% as.character.Date(MSFT$BV$Date) ,11] <- 
  as.numeric(rev(MSFT$BV$EPS_ttm))
MSFT$Data[as.character.Date(MSFT$Data[,1]) %in% as.character.Date(MSFT$BV$Date) ,12] <- 
  as.numeric(rev(MSFT$BV$d.BPS_E))
MSFT$Data[as.character.Date(MSFT$Data[,1]) %in% as.character.Date(MSFT$BV$Date) ,13] <- 
  as.numeric(rev(MSFT$BV$d.EPS_ttm))

MSFT$Data$d.BPS_E[is.nan(MSFT$Data$d.BPS_E)] <- 0
MSFT$Data$d.BPS_E[is.infinite(MSFT$Data$d.BPS_E)] <- 0

MSFT$Data$d.EPS_ttm[is.nan(MSFT$Data$d.EPS_ttm)] <- 0
MSFT$Data$d.EPS_ttm[is.infinite(MSFT$Data$d.EPS_ttm)] <- 0
```
  
Die nicht aufgezeichneten Zeitpunkte (per Definition alle Punkte mit Wert = 0) werden dann "geforwarded", sprich durch die letzte aufgezeichnete Beobachtung ersetzt.
  
```{r MSFT Import 4}
for(i in c(2:count_of_days)){
  if(MSFT$Data[i, 2] == 0) MSFT$Data[i, 2] <- MSFT$Data[i-1, 2]
  if(MSFT$Data[i, 9] == 0) MSFT$Data[i, 9] <- MSFT$Data[i-1, 9]
  if(MSFT$Data[i, 10] == 0) MSFT$Data[i, 10] <- MSFT$Data[i-1, 10]
  if(MSFT$Data[i, 11] == 0) MSFT$Data[i, 11] <- MSFT$Data[i-1, 11]
  if(MSFT$Data[i, 12] == 0) MSFT$Data[i, 12] <- MSFT$Data[i-1, 12]
  if(MSFT$Data[i, 13] == 0) MSFT$Data[i, 13] <- MSFT$Data[i-1, 13]
}
```
  
Es geht weiter mit den spezifischen Daten des Preises. Hier werden zusätzliche Variablen wie die Veränderung (immer mit d.* gekennzeichnet) und der MA berechnet. Diese werden in einem xts-Objekt zusammengefasst. Das dient der besseren Darstellbarkeit und Übersicht. Die Veränderung wird als prozentuale Veränderung zum Vortag berechnet:

$$ d.X_{t} =  \frac{X_{t} - X_{t-1}}{X_{t-1}}$$  
  
Die Veränderung wird auf Basis der ersten geglätteten Zeitreihe der Preiszeitreihe berechnet. Sinn davon ist, dass kurzfristige Effekte entfernt werden. Somit verbleibt dann nur noch die Niveau- und Trendkomponente.
Es folgt nun eine kurze Übersicht über die Daten und berechneten Datensätze.  
```{r MSFT Import 5}
MSFT$P_xts <- xts(MSFT$Data$PPS, order.by = MSFT$Data$Date)
colnames(MSFT$P_xts) <- c("P")
#MSFT$P_xts$MA <- xts(ma(MSFT$P_xts$P, 181), order.by = MSFT$Data$Date)
#MSFT$P_xts$MA2 <- xts(ma(MSFT$P_xts$P, 913), order.by = MSFT$Data$Date)

MSFT$P_xts$MA <- xts(c(rep(NA, 89), rollmeanr(MSFT$P_xts$P, 90)), order.by = MSFT$Data$Date)
#MSFT$P_xts$MA <- xts(ma(MSFT$P_xts$P, 90), order.by = MSFT$Data$Date)
MSFT$P_xts$d.P <- (MSFT$P_xts$P - lag(MSFT$P_xts$P, 1))/lag(MSFT$P_xts$P, 1)
MSFT$P_xts$d.P[is.nan(MSFT$P_xts$d.P)] <- NA
MSFT$P_xts$d.P[is.infinite(MSFT$P_xts$d.P)] <- NA
MSFT$P_xts$d.P_MA <- (MSFT$P_xts$MA - lag(MSFT$P_xts$MA, 1))/lag(MSFT$P_xts$MA, 1)
MSFT$P_xts$d.P_MA[is.nan(MSFT$P_xts$d.P_MA)] <- NA
MSFT$P_xts$d.P_MA[is.infinite(MSFT$P_xts$d.P_MA)] <- NA


plot(MSFT$P_xts, col = c("black", "red", "green"), main = c("Price"))
plot(MSFT$P_xts$d.P, main = c("Change of Price"))
plot(MSFT$P_xts$d.P_MA, main = c("Change of MA of Price"))
plot.dens(dataset = MSFT$P_xts$d.P_MA, a = 2, title = c("Density of Change in Price"), plot.norm = TRUE)
plot.ext(MSFT$P_xts$d.P_MA, a = 2, title = c("Extrems of change in Prices"))
```
  
Man beachte hier, dass der MA ein backwardsfacing rolling mean ist. Im gegensatz zum symetrischen MA, kennt der backwardsfacing MA, den zukünftigen verlauf der Zeitreihe nicht, er reagiert also auf Änderungen der Zeitreihe erst, wenn diese auch Eintritt. Er eignet sich somit für alle arten von Forecasting.  
  
Ähnliches geschieht nun mit den Buchwertdaten (BV) und den Einkommensdaten (NI = Net Income). Auch diese werden wieder in getrennten xts-Objekten abgespeichert.
Man beachte, dass die Buchwertdaten jetzt erst von ihren "Lücken" bereinigt werden. Nicht zugewiesene Beobachtungen werden Interpoliert.
  
```{r MSFT Import 6}
#BV---------------------------------------------------------------------------
MSFT$BV_xts <- xts(MSFT$Data$BPS_E, order.by = MSFT$Data$Date)
colnames(MSFT$BV_xts) <- c("BPS_E")
MSFT$BV_xts$BPS_E[MSFT$BV_xts$BPS_E == 0] <- NA
MSFT$BV_xts$BPS_E[1] <- 0
MSFT$BV_xts$BPS_E[BV_start_date] <- 0
MSFT$BV_xts$BPS_E <- xts(na.interp(MSFT$BV_xts$BPS_E), order.by = MSFT$Data$Date)
MSFT$BV_xts$BPS_E[BV_start_date] <- NA

MSFT$BV_xts$d.BPS_E <- xts(MSFT$Data$d.BPS_E, order.by = MSFT$Data$Date)
MSFT$BV_xts$d.EPS_ttm <- xts(MSFT$Data$d.EPS_ttm, order.by = MSFT$Data$Date)
#MSFT$BV_xts$d.BPS_E[is.nan(MSFT$BV_xts$d.BPS_E)] <- NA
#MSFT$BV_xts$d.BPS_E[is.na(MSFT$BV_xts$d.BPS_E)] <- 0
#MSFT$BV_xts$d.BPS_E[is.infinite(MSFT$BV_xts$d.BPS_E)] <- NA

MSFT$BV_xts$BPS_D <- xts(MSFT$Data$BPS_D, order.by = MSFT$Data$Date)
MSFT$BV_xts$BPS_D[MSFT$BV_xts$BPS_D == 0] <- NA
MSFT$BV_xts$BPS_D[1] <- 0
MSFT$BV_xts$BPS_D[BV_start_date] <- 0
MSFT$BV_xts$BPS_D <- xts(na.interp(MSFT$BV_xts$BPS_D), order.by = MSFT$Data$Date)

plot(MSFT$BV_xts$BPS_E, main = c("Equity Book Value"))


#NI---------------------------------------------------------------------------
MSFT$EPS_xts <- xts(MSFT$Data$EPS, order.by = MSFT$Data$Date)
colnames(MSFT$EPS_xts) <- c("EPS")
MSFT$EPS_xts$EPS[MSFT$EPS_xts$EPS <= 0] <- NA

MSFT$EPS_xts$EPS_ttm <- xts(MSFT$Data$EPS_ttm, order.by = MSFT$Data$Date)
MSFT$EPS_xts$EPS_ttm[MSFT$EPS_xts$EPS_ttm <= 0] <- NA
```
  
### Multiples {.tabset .tabset-pills}
#### P/B Ratio
  
Kommen wir nun zur Berechnung der Multiples. Begonnen wird mit dem P/B Ratio:
  
$$ PB_{t} = \frac{P_{t}}{BPS^{E}_{t}} $$ 
$BPS^{E}$ ist dabei der BPS des Equity (BPS_E).

```{r MSFT Import 7}
MSFT$Ratios.PB <- xts(MSFT$P_xts$MA / MSFT$BV_xts$BPS_E, order.by = MSFT$Data$Date)
colnames(MSFT$Ratios.PB) <- c("PB")
MSFT$Ratios.PB$PB[MSFT$Ratios.PB$PB == Inf] <- 0

MSFT$Ratios.PB$MA <- xts(ma(MSFT$Ratios.PB$PB, 540), order.by = MSFT$Data$Date)
#MSFT$Ratios.PB$MA <- xts(c(rep(NA, 539), rollmeanr(MSFT$Ratios.PB$P, 540)), order.by = MSFT$Data$Date)
MSFT$Ratios.PB$MA2 <- xts(ma(MSFT$Ratios.PB$PB, 360), order.by = MSFT$Data$Date)
#MSFT$Ratios.PB$MA2 <- xts(c(rep(NA, 99), rollmeanr(MSFT$Ratios.PB$PB, 100)), order.by = MSFT$Data$Date)

MSFT$Ratios.PB$NivCleanded <- MSFT$Ratios.PB$PB - MSFT$Ratios.PB$MA
MSFT$Ratios.PB$d.PB <- (MSFT$Ratios.PB$PB - lag(MSFT$Ratios.PB$PB, 1))/lag(MSFT$Ratios.PB$PB, 1)
MSFT$Ratios.PB$d.PB[is.nan(MSFT$Ratios.PB$d.PB)] <- NA
MSFT$Ratios.PB$d.PB[is.infinite(MSFT$Ratios.PB$d.PB)] <- NA

plot(MSFT$Ratios.PB, main = c("PB Ratio"))
```
  
Hier lässt sich leider kein rolling mean verwenden, sondern nur der symetrische MA. Das macht eine Prognose des Modells vorerst unmöglich (oder zumindest ziemlich ungenau). Evtl. bräuchte man hier noch eine bessere Idee um das Niveau des PBs zu bestimmen. Alternative Methodiken die bereits getestet wurden:  

* Polynomialer Trend
* ARMA mit sehr kleinen Parametern
* rolling mean  

Alternativen die noch getested werden müssen:  

* eine kombination von mehreren rolling means, mit unterschiedlichen längen, um kurz und langfristige Trends im Niveau ab zu fangen
* Konstrucktion eines Schwingugnsintervalls mithilfe der verteilung der Änderungen
* ein Stochastisches Modell, welches jede Änderung mit einer Wahrscheinlichkeit gewichted
  
A.d.A.: Mir gefällt der MA an der Stelle sowieso noch nicht. Extrem hohe Werte des PBs werden zwar unterschätzt (und gleichzeitig extrem niedrige Werte überschätzt), was so auch beabsichtig ist. Der MA (egal welcher Art) hat allerdings keine Strafe für extreme, niveautechnische Ausreißer (also besonders hohe und besonders niedrige Werte). Was ich damit meine ist, dass eine Funktion gesucht ist, die (eine umso stärkere dämpfung hat)in sachen Anpassung) um so stärker gedämpft ist, je höher ihr aktuelles Niveau ist. Um einen solchen Effekt, mit einem MA, zu Simulieren, müsste man eine noch viel größere Breite des MAs benutzen als ich es getan habe. 

Das Niveau welches geschätzt wird, wird nun von dem PB abgezogen, um eine Art Signal Zeitreihe zu erhalten. Sinn dieser ist es, dass PB, welche (weit) überhalb des aktuellen Niveau de PBs liegen, auf eine Blase oder eine überbewertung am Markt hindeuten könnten.
  
#### P/E Ratio
  
$$ PE_{t} = \frac{P_{t}}{EPSttm_{t}} $$

```{r MSFT Import 8}
MSFT$Ratios.PE <- xts((MSFT$P_xts$MA + MSFT$BV_xts$BPS_D)/MSFT$EPS_xts$EPS_ttm, order.by = MSFT$Data$Date)
colnames(MSFT$Ratios.PE) <- c("PE")
MSFT$Ratios.PE$PE[MSFT$Ratios.PE$PE == Inf] <- 0

#MSFT$Ratios.PE$MA <- xts(c(rep(NA, 199), rollmeanr(MSFT$Ratios.PE$PE, 200)), order.by = MSFT$Data$Date)
MSFT$Ratios.PE$MA <- xts(ma(MSFT$Ratios.PE$PE, 540), order.by = MSFT$Data$Date)
#MSFT$Ratios.PE$MA2 <- xts(c(rep(NA, 99), rollmeanr(MSFT$Ratios.PE$PE, 100)), order.by = MSFT$Data$Date)
MSFT$Ratios.PE$MA2 <- xts(ma(MSFT$Ratios.PE$PE, 360), order.by = MSFT$Data$Date)


MSFT$Ratios.PE$d.PE <- (MSFT$Ratios.PE$PE - lag(MSFT$Ratios.PE$PE, 1))/lag(MSFT$Ratios.PE$PE, 1)
MSFT$Ratios.PE$d.PE[is.nan(MSFT$Ratios.PE$d.PE)] <- NA
MSFT$Ratios.PE$d.PE[is.infinite(MSFT$Ratios.PE$d.PE)] <- NA

plot(MSFT$Ratios.PE, main = c("PE Ratio"))
```
  
## 3) Aufstellen des ersten Modells
### Univariate Regression {.tabset .tabset-pills}
#### Berechnung
  
Annahme ist, dass eine Abhängigkeit zwischen dem veränderung des Preises und dem Niveau des PB Ratios besteht. Deswegen liegt die wahl des ersten Modelles relativ nah.
Modell:
$$ d.P_{t, i} = \beta_{0, i} + \beta_{1, i} * niv.PB_{t-i} + e_{i, t} $$ 
mit $t \in T$.  
  
$d.P_{t}$ ist dabei die Veränderung des 90 Tage MAs des Preises (Berechnung siehe oben in der Datenvorbereitung). $niv.PB_{t}$ beschreibt das PB, um ein gewisses Niveau bereinigt. Die Berechnung dazu, siehe oben.

  
Das Modell wird für alle $i \in [1;730]$ geschätzt. Dabei werden die Kennzahlen AIC und BIC aufgezeichnet. Zur Bestimmung des optimalen Modells, wird das i mit dem geringsten AIC bzw. BIC selektiert.
  
```{r MSFT simple PB Model}
MSFT$NivClean_Ind <- index(MSFT$Ratios.PB$NivCleanded[!is.na(MSFT$Ratios.PB$NivCleanded)])

PB_cor <- data.frame(matrix(NA, nrow = 730, ncol = 2))
colnames(PB_cor) <- c("AIC", "BIC")

data.xts <- xts(MSFT$Ratios.PB$NivCleanded[MSFT$NivClean_Ind], order.by = MSFT$NivClean_Ind)
colnames(data.xts) <- c("PB")
data.xts$P <- MSFT$P_xts$d.P_MA[MSFT$NivClean_Ind]

for(i in c(1:730)){
  model <- lm(data.xts$P ~ lag(data.xts$PB, i))
  PB_cor$AIC[i] <- AIC(model)
  PB_cor$BIC[i] <- BIC(model)
}


(best_i <- order(PB_cor$AIC)[1])
(order(PB_cor$BIC)[1])


par(mfrow = c(1, 1))
plot(PB_cor$AIC, type = c("l"), main = c("AIC and BIC"))
lines(PB_cor$BIC, col = c("orange"))
abline(v = best_i)
```
  
Anschließend wird das Modell für das beste i neu berechnet:
  
```{r MSFT simple PB model fitting}
data.xts$PB <- lag(MSFT$Ratios.PB$NivCleanded[MSFT$NivClean_Ind], 65)
model <- lm(data.xts$P ~ data.xts$PB, na.action = na.exclude)
summary(model)

AIC(model)
BIC(model)

plot(ts(MSFT$P_xts$d.P_MA[MSFT$NivClean_Ind]), main = c("Fitted Values"))
lines(c(rep(NA, 65), ts(model$fitted.values)), col = c("red"))
```
  
#### Betrachtung der Residuen
Kurze Betrachtung der Residuen des Modells:
  
```{r MSFT simple PB model residuals}
plot.dens(as.numeric(model$residuals), title = c("Density of Residuals"), plot.norm = TRUE, plot.lines = FALSE)
```
  
Wie man sieht, sind sie nicht perfekt normal Verteilt. 
Ein weiteres Problem, welches sich häufig bei Finanzdaten ergibt, sind Autokorrelationen in den Residuen.
  
```{r MSFT simple PB model residuals 2}
par(mfrow = c(1, 2))
acf(as.numeric(model$residuals[!is.na(model$residuals)]), main = c("ACF: Residuals"))
pacf(as.numeric(model$residuals[!is.na(model$residuals)]), main = c("PACF: Residuals"))
par(mfrow = c(1, 1))
```
  
Die langsam abklingende ACF kann auf zwei Dinge hindeuten: einen AR-Prozess oder einen Random Walk Prozess.
Im folgenden werden die beiden Möglichkeiten mithilfe von auto.arima() verglichen.
A.d.A.: Zuvor hatten wir ja, neben der abklingenden ACF, auch noch schwingende Strukturen in der PACF gesehen. Diesen bin ich Herr geworden, indem ich bei der Preis Zeitreihe statt einem normalen MA, einen backwardsfacing rolling mean verwendet habe.
  
```{r MSFT simple PB model residuals 3}
(model.autoarima <- auto.arima(ts(model$residuals[!is.na(model$residuals)])))
```
  
Die auto.arima() Funktion testet mithilfe des ADF Test ob eine Einheitswurzel vorliegt. Nachdem es einen ARIMA(0, 1, 1) Prozess vorschlägt, liegt die Vermutung eines Random Walks sehr nahe. Trotzdem wird als zweiter Ansatz nun ein AR Prozess gefitted, indem auto.arima dazu gezwungen wird ein ARMA Modell zu fitten.
  
```{r MSFT simple PB model residuals 4}
(model.stationary <- auto.arima(ts(model$residuals[!is.na(model$residuals)]), d = 0))
```
  
Die auto.arima() Funktion schlägt in diesem Fall ein ARMA(2, 0) oder ein AR(2) Prozess vor. Beide Möglichkeiten werden im folgenden nun genauer untersucht.
  
### ADL

Es wird der Gedanke des einfachen Modells, sprich dass sich der Preis durch eine  
Als ersten Versuch, wurde ein ADL Modell mit einem AR(2) Prozess in den Residuen hergeleitet.
$$P_{t} = \beta_{0} + \beta_{1} * PB_{t} + u_{t} $$
$$u_{t} = \gamma_{1} * u_{t-1} + \gamma_{2} * u_{t-2} + e_{t}$$
  
Nach dem Umformen ergibt dies ein ADL(2, 2) Modell:

$$ P_{t} = \lambda_{0} + \lambda_{1} * P_{t-1} + \lambda_{2} * P_{t-2} + \psi_{1} * PB_{t} + \psi_{2} + PB_{t-1} + \psi_{3} * PB_{t-2} + e_{t} $$

mit

$$ e_{t} \sim {WN} $$
und

$\lambda_{0} = (1 - \gamma_{1} - \gamma_{2})$ , $\lambda_{1} = \gamma_{1}$ , $\lambda_{2} = \gamma_{2}$ , $\psi_{1} = \beta_{1}$ , $\psi_{2} = -\gamma_{1} * \beta_{1}$ , $\psi_{3} = -\gamma_{2}*\beta_{1}$.
  
#### Vorraussetzungen:
Damit das ADL Modell gefitted werden kann, muss zuerst sichergestellt werden, dass die Variablen Stationär sind. Hierzu wird der ADF - Test, sowie die ACF Funktion heran gezogen.
Es wird die adf.test() Funktion aus dem tseries Packet verwendet.
  
```{r MSFT ADL Model preperation}
MSFT$NivClean_Ind <- index(MSFT$Ratios.PB$NivCleanded[!is.na(MSFT$Ratios.PB$NivCleanded)])

AIC_BIC <- data.frame(matrix(NA, nrow = 730, ncol = 2))
colnames(AIC_BIC) <- c("AIC", "BIC")

data.xts <- xts(MSFT$Ratios.PB$NivCleanded[MSFT$NivClean_Ind], order.by = MSFT$NivClean_Ind)
colnames(data.xts) <- c("PB")
data.xts$P <- MSFT$P_xts$d.P_MA[MSFT$NivClean_Ind]

#Test for the stationary requirement of P and PB
par(mfrow = c(2, 2))
acf(data.xts$P, main = c("ACF: P"))
pacf(data.xts$P, main = c("PACF: P"))
acf(data.xts$PB, main = c("ACF: PB"))
pacf(data.xts$PB, main = c("PACF: PB"))
```
  
Die Plots legen nahe, dass keine Stationarität vorliegt. 
  
```{r MSFT ADL Model Preperation ADF Test P}
adf.test(data.xts$P)
```
  
Der ADF Test legt nahe, dass Stationarität vorliegt. 
  
```{r MSFT ADL Model preperation ADF Test PB}
adf.test(data.xts$PB)
```
  
Auch hier wird die Nullhypothes des ADF-Tests abgelehnt, was bedeutet, dass keine Einheitswurzel vorliegt. Ein Differenzieren der Daten ist demnach nicht nötig.
   
Als nächstes wird das Modell gefitted:
  
```{r MSFT ADL Model fitting}
model <- lm(data.xts$P ~ data.xts$PB + lag(data.xts$PB, 1) + lag(data.xts$PB, 2) + lag(data.xts$P, 1) + lag(data.xts$P, 2))
summary(model)
AIC(model)
BIC(model)
```
  
Man beachte das sehr hohe R-squared und die beiden Kennzahlen AIC und BIC, welche wesentlich niedriger sind als bei den Vorherigen Modellen.
Es folgt die Betrachtung der Residuen.
  

```{r}
plot.dens(as.numeric(model$residuals), title = c("Density of Residuals"), plot.norm = TRUE, plot.lines = FALSE)
par(mfrow = c(1, 2))
acf(model$residuals, main = c("ACF: Residuen"))
pacf(model$residuals, main = c("PACF: Residuen"))
par(mfrow = c(1, 1))
```
  
Sie sind wieder nicht normal verteilt, die ACF und die PACF legen aber die Vermutung nahe, dass es keine vergessene Systematik in den Residuen gibt.
  
```{r, fig.width= 9, fig.height=6}
plot.data <- data.xts$P
colnames(plot.data) <- c("P")
plot.data$fit <- model$fitted.values
plot(plot.data["1990/2000"], type = c("l"), col = c("black", "orange", "blue"), main = c("Fitted Values"))
plot(plot.data["2000/2010"], type = c("l"), col = c("black", "orange", "blue"), main = c("Fitted Values"))
plot(plot.data["2010/2019"], type = c("l"), col = c("black", "orange", "blue"), main = c("Fitted Values"))
```
  
# Vorstellung der Funktionen aus functions.R  
  
```{r}
plot.dens <- function(dataset, a = 2, title, color = c("black", "black", "black"), plot.norm = FALSE, plot.lines = TRUE){
  m <- mean(dataset, na.rm = TRUE)
  s <- sd(dataset, na.rm = TRUE)
  pp <- plot(density(dataset, na.rm = TRUE), main = title, col = color[1])
  if(plot.lines){
    pp <- abline(v = m + a*s, col = color[2])
    pp <- abline(v = m - a*s, col = color[3])
  }
  if(plot.norm){
    pp <- lines(density(rnorm(n = 1000000, mean = m, sd = s)), col = c("grey"))
  }
  invisible(pp)
}

```
  
Diese Funktion plottet die Verteilung eines Datensatzes und zeichnet zwei Linien bei der a-fachen Standardabweichung ein.  
  
```{r}
plot.ext <- function(dataset, a = 2, title, color = c("black", "green", "orange")){
  m <- mean(dataset, na.rm = TRUE)
  s <- sd(dataset, na.rm = TRUE)
  ind_up <- index(dataset[dataset > (m + a * s)])
  ind_down <- index(dataset[dataset < (m - a * s)])
  plot_data <- dataset
  colnames(plot_data) <- c("a")
  plot_data$b <- dataset
  plot_data$c <- dataset
  plot_data$a[index(plot_data) %in% ind_up] <- NA
  plot_data$a[index(plot_data) %in% ind_down] <- NA
  plot_data$b[!index(plot_data) %in% ind_up] <- NA
  plot_data$c[!index(plot_data) %in% ind_down] <- NA
  plot(plot_data, col = color, type = c("p"), main = title)
}
```
  
Diese Funktion plottet den Datensatz und markiert alle Werte mit einer Abweichung die größer ist als die a-fache Standardabweichung.  
  
# Auswertung bei weiteren Firmen {.tabset .tabset-pills}
## Apple
### Import
  
```{r}
AAPL <- new.env()
AAPL$MV <- read_excel("Data_Eikon/American_Electronics/Apple.xlsx", 
                      sheet = "Prices", col_types = c("date", 
                                                      "numeric"))

AAPL$BV <- read_excel("Data_Eikon/American_Electronics/Apple.xlsx", 
                      sheet = "Data", col_types = c("date", 
                                                    "numeric", "numeric", "numeric", 
                                                    "numeric", "numeric", "numeric", "numeric", "numeric"))
P_st_da <- "1980-12-12"
BV_st_da <- "/1989-09-28"
day_c <- 13934
import(AAPL, P_start_date = P_st_da, BV_start_date = BV_st_da, count_of_days = day_c)
```
  
Die Funktion Import übernimmt den gesamten Import und die Vorbereitung der Daten. Die komplette Funktion wurde am Beispiel von Microsoft weiter oben durchgesprochen.
  
### Überblick
  
```{r , fig.width = 9, fig.height = 6}
par(mfrow = c(2,2))
plot(AAPL$P_xts, col = c("black", "red", "green"), main = c("AAPL: Price"))
plot(AAPL$BV_xts$BPS_E, main = c("Equity Book Value"))
plot(AAPL$Ratios.PB, main = c("AAPL: PB Ratio"))
plot(AAPL$Ratios.PE, main = c("AAPL: PE Ratio"))


plot(AAPL$P_xts$d.P, main = c("Change of Price"))
plot(AAPL$P_xts$d.P_MA, main = c("Change of MA of Price"))
plot.dens(AAPL$P_xts$d.P_MA, a = 2, title = c("Density of change in Price"), plot.norm = TRUE)

plot.ext(AAPL$P_xts$d.P_MA, a = 2, title = c("Extrem Values in the change of Prices"))
par(mfrow = c(1,1))
```
  
### Auswertung
#### Univariates Model
##### Berechnung:
  
```{r}
AAPL$NivClean_Ind <- index(AAPL$Ratios.PB$NivCleaned[!is.na(AAPL$Ratios.PB$NivCleaned)])

PB_cor <- data.frame(matrix(NA, nrow = 730, ncol = 2))
colnames(PB_cor) <- c("AIC", "BIC")

data.xts <- xts(AAPL$Ratios.PB$NivCleaned[AAPL$NivClean_Ind], order.by = AAPL$NivClean_Ind)
colnames(data.xts) <- c("PB")
data.xts$P <- AAPL$P_xts$d.P_MA[AAPL$NivClean_Ind]

#find the best fitting model
for(i in c(1:730)){
  model <- lm(data.xts$P ~ lag(data.xts$PB, i))
  PB_cor$AIC[i] <- AIC(model)
  PB_cor$BIC[i] <- BIC(model)
}

(best_i <- order(PB_cor$AIC)[1])
(order(PB_cor$BIC)[1])

par(mfrow = c(1, 1))
plot(PB_cor$AIC, type = c("l"), main = c("AIC and BIC"))
lines(PB_cor$BIC, col = c("orange"))
abline(v = best_i)
```
  
Man beachte hier, dass das optimale i 70 ist und nicht wie davor 65. Es wird dennoch weiter mit 65 gerechnet.
  
```{r}
data.xts$PB <- lag(AAPL$Ratios.PB$NivCleaned[AAPL$NivClean_Ind], 65)
model <- lm(data.xts$P ~ data.xts$PB, na.action = na.exclude)
summary(model)

AIC(model)
BIC(model)

plot(ts(AAPL$P_xts$d.P_MA[AAPL$NivClean_Ind]), main = c("Fitted Values"))
lines(c(rep(NA, 65), ts(model$fitted.values)), col = c("red"))
```
  
##### Betrachtung der Residuen:
  
```{r}
plot.dens(as.numeric(model$residuals), title = c("Density of Residuals"), plot.norm = TRUE, plot.lines = FALSE)

par(mfrow = c(1, 2))
Acf(as.numeric(model$residuals[!is.na(model$residuals)]), main = c("ACF: Residuals"))
Pacf(as.numeric(model$residuals[!is.na(model$residuals)]), main = c("PACF: Residuals"))
par(mfrow = c(1, 1))

(model.autoarima <- auto.arima(ts(model$residuals[!is.na(model$residuals)])))

(model.stationary <- auto.arima(ts(model$residuals[!is.na(model$residuals)]), d = 0))
```
  
Auch hier ergibt sich wieder ein AR Prozess oder eine Einheitswurzel in den Residuen. Sie sind wieder nicht perfekt Normal verteilt.
  
#### ADL
##### Berechnung:
Es wird das selbe ADL Modell wie bei Microsoft benutzt.
  
```{r, fig.width= 9, fig.height=6}
AAPL$NivClean_Ind <- index(AAPL$Ratios.PB$NivCleaned[!is.na(AAPL$Ratios.PB$NivCleaned)])

AIC_BIC <- data.frame(matrix(NA, nrow = 730, ncol = 2))
colnames(AIC_BIC) <- c("AIC", "BIC")

data.xts <- xts(AAPL$Ratios.PB$NivCleaned[AAPL$NivClean_Ind], order.by = AAPL$NivClean_Ind)
colnames(data.xts) <- c("PB")
data.xts$P <- AAPL$P_xts$d.P_MA[AAPL$NivClean_Ind]

#Test for the stationary requirement of P and PB
par(mfrow = c(2, 2))
acf(data.xts$P, main = c("ACF: P"))
pacf(data.xts$P, main = c("PACF: P"))
acf(data.xts$PB, main = c("ACF: PB"))
pacf(data.xts$PB, main = c("PACF: PB"))

adf.test(data.xts$P)
adf.test(data.xts$PB)

#Fitting of the Model
model <- lm(data.xts$P ~ data.xts$PB + lag(data.xts$PB, 1) + lag(data.xts$PB, 2) + lag(data.xts$P, 1) + lag(data.xts$P, 2))
summary(model)
AIC(model)
BIC(model)

par(mfrow = c(1, 1))
plot.data <- data.xts$P
colnames(plot.data) <- c("P")
plot.data$fit <- model$fitted.values
plot(plot.data["1990/2000"], type = c("l"), col = c("black", "orange", "blue"), main = c("Fitted Values"))
plot(plot.data["2000/2010"], type = c("l"), col = c("black", "orange", "blue"), main = c("Fitted Values"))
plot(plot.data["2010/2019"], type = c("l"), col = c("black", "orange", "blue"), main = c("Fitted Values"))
```
  
##### Betrachtung der Residuen:
  
```{r}
plot.dens(as.numeric(model$residuals), title = c("Density of Residuals"), plot.norm = TRUE, plot.lines = FALSE)
par(mfrow = c(1, 2))
acf(model$residuals, main = c("ACF: Residuen"))
pacf(model$residuals, main = c("PACF: Residuen"))
par(mfrow = c(1, 1))
```

  
## IBM
### Import
  
```{r}
IBM <- new.env()

IBM$MV <- read_excel("Data_Eikon/American_Electronics/IBM.xlsx", 
                     sheet = "Prices", col_types = c("date", 
                                                     "numeric"))

IBM$BV <- read_excel("Data_Eikon/American_Electronics/IBM.xlsx", 
                     sheet = "Data", col_types = c("date", 
                                                   "numeric", "numeric", "numeric", 
                                                   "numeric", "numeric", "numeric", "numeric", "numeric"))

P_st_da <- "1980-03-17"
BV_st_da <- "/1989-12-30"
day_c <- 14204
import(IBM, P_start_date = P_st_da, BV_start_date = BV_st_da, count_of_days = day_c)
```
  
### Überblick
  
```{r , fig.width = 9, fig.height = 6}
par(mfrow = c(2,2))
plot(IBM$P_xts, col = c("black", "red", "green"), main = c("IBM: Price"))
plot(IBM$BV_xts$BPS_E, main = c("IBM: BPS"))
plot(IBM$Ratios.PB, main = c("IBM: PB"))
plot(IBM$Ratios.PE, main = c("PE Ratio"))

plot(IBM$P_xts$d.P, main = c("Change of Price"))
plot(IBM$P_xts$d.P_MA, main = c("Change of MA of Price"))
plot.dens(IBM$P_xts$d.P_MA, a = 2, title = c("Density of change in Price"), plot.norm = TRUE)

plot.ext(IBM$P_xts$d.P_MA, a = 2, title = c("Extrems of change in Prices"))
par(mfrow = c(1,1))
```
  
### Auswertung
#### Univariates Model
##### Berechnung:
  

```{r}
IBM$NivClean_Ind <- index(IBM$Ratios.PB$NivCleaned[!is.na(IBM$Ratios.PB$NivCleaned)])

PB_cor <- data.frame(matrix(NA, nrow = 730, ncol = 2))
colnames(PB_cor) <- c("AIC", "BIC")

data.xts <- xts(IBM$Ratios.PB$NivCleaned[IBM$NivClean_Ind], order.by = IBM$NivClean_Ind)
colnames(data.xts) <- c("PB")
data.xts$P <- IBM$P_xts$d.P_MA[IBM$NivClean_Ind]

#find the best fitting model
for(i in c(1:730)){
  model <- lm(data.xts$P ~ lag(data.xts$PB, i))
  PB_cor$AIC[i] <- AIC(model)
  PB_cor$BIC[i] <- BIC(model)
}

(best_i <- order(PB_cor$AIC)[1])
(order(PB_cor$BIC)[1])

par(mfrow = c(1, 1))
plot(PB_cor$AIC, type = c("l"), main = c("AIC and BIC"))
lines(PB_cor$BIC, col = c("orange"))
abline(v = best_i)

#fitting of the best the model with the lowest AIC/BIC
data.xts$PB <- lag(IBM$Ratios.PB$NivCleaned[IBM$NivClean_Ind], 65)
model <- lm(data.xts$P ~ data.xts$PB, na.action = na.exclude)
summary(model)

AIC(model)
BIC(model)

plot(ts(IBM$P_xts$d.P_MA[IBM$NivClean_Ind]), main = c("Fitted Values"))
lines(c(rep(NA, 65), ts(model$fitted.values)), col = c("red"))
```
  
##### Betrachtung der Residuen:
  
```{r}
plot.dens(as.numeric(model$residuals), title = c("Density of Residuals"), plot.norm = TRUE, plot.lines = FALSE)

par(mfrow = c(1, 2))
Acf(as.numeric(model$residuals[!is.na(model$residuals)]), main = c("ACF: Residuals"))
Pacf(as.numeric(model$residuals[!is.na(model$residuals)]), main = c("PACF: Residuals"))
par(mfrow = c(1, 1))

(model.autoarima <- auto.arima(ts(model$residuals[!is.na(model$residuals)])))

(model.stationary <- auto.arima(ts(model$residuals[!is.na(model$residuals)]), d = 0))

```
  
#### ADL
##### Berechnung:
  
```{r, fig.width= 9, fig.height=6}
IBM$NivClean_Ind <- index(IBM$Ratios.PB$NivCleaned[!is.na(IBM$Ratios.PB$NivCleaned)])

AIC_BIC <- data.frame(matrix(NA, nrow = 730, ncol = 2))
colnames(AIC_BIC) <- c("AIC", "BIC")

data.xts <- xts(IBM$Ratios.PB$NivCleaned[IBM$NivClean_Ind], order.by = IBM$NivClean_Ind)
colnames(data.xts) <- c("PB")
data.xts$P <- IBM$P_xts$d.P_MA[IBM$NivClean_Ind]

#Test for the stationary requirement of P and PB
par(mfrow = c(2, 2))
acf(data.xts$P, main = c("ACF: P"))
pacf(data.xts$P, main = c("PACF: P"))
acf(data.xts$PB, main = c("ACF: PB"))
pacf(data.xts$PB, main = c("PACF: PB"))

adf.test(data.xts$P)
adf.test(data.xts$PB)

#Fitting of the Model
model <- lm(data.xts$P ~ data.xts$PB + lag(data.xts$PB, 1) + lag(data.xts$PB, 2) + lag(data.xts$P, 1) + lag(data.xts$P, 2))
summary(model)
AIC(model)
BIC(model)

par(mfrow = c(1, 1))
plot.data <- data.xts$P
colnames(plot.data) <- c("P")
plot.data$fit <- model$fitted.values
plot(plot.data["1990/2000"], type = c("l"), col = c("black", "orange", "blue"), main = c("Fitted Values"))
plot(plot.data["2000/2010"], type = c("l"), col = c("black", "orange", "blue"), main = c("Fitted Values"))
plot(plot.data["2010/2019"], type = c("l"), col = c("black", "orange", "blue"), main = c("Fitted Values"))
```
  
##### Betrachtung der Residuen:
  
```{r}
plot.dens(as.numeric(model$residuals), title = c("Density of Residuals"), plot.norm = TRUE, plot.lines = FALSE)
par(mfrow = c(1, 2))
acf(model$residuals, main = c("ACF: Residuen"))
pacf(model$residuals, main = c("PACF: Residuen"))
par(mfrow = c(1, 1))
```

  
## Oracle
### Import
  
```{r}
ORCL <- new.env()

ORCL$MV <- read_excel("Data_Eikon/American_Electronics/Oracle.xlsx", 
                      sheet = "Prices", col_types = c("date", 
                                                      "numeric"))

ORCL$BV <- read_excel("Data_Eikon/American_Electronics/Oracle.xlsx", 
                      sheet = "Data", col_types = c("date", 
                                                    "numeric", "numeric", "numeric", 
                                                    "numeric", "numeric", "numeric", "numeric", "numeric"))

P_st_da <- "1986-03-12"
BV_st_da <- "/1990-05-30"
day_c <- 12018
import(ORCL, P_start_date = P_st_da, BV_start_date = BV_st_da, count_of_days = day_c)
```
  
### Überblick
  
```{r ORCL_Overview, fig.width = 9, fig.height = 6}
par(mfrow = c(2,2))
plot(ORCL$P_xts, col = c("black", "red", "green"), main = c("ORCL: Price"))
plot(ORCL$BV_xts$BPS_E, main = c("ORCL: BPS"))
plot(ORCL$Ratios.PB, main = c("ORCL: PB"))
plot(ORCL$Ratios.PE, main = c("PE Ratio"))

plot(ORCL$P_xts$d.P, main = c("ORCL: Change of Price"))
plot(ORCL$P_xts$d.P_MA, main = c("ORCL: Change of MA of Price"))
plot.dens(ORCL$P_xts$d.P_MA, a = 2, title = c("ORCL: Density of change in Price"), plot.norm = TRUE)

plot.ext(ORCL$P_xts$d.P_MA, a = 2, title = c("ORCL: Extrems of change in Prices"))
par(mfrow = c(1,1))
```
  
### Auswertung

#### Univariates Modell
##### Berechnung:
  
```{r}
ORCL$NivClean_Ind <- index(ORCL$Ratios.PB$NivCleaned[!is.na(ORCL$Ratios.PB$NivCleaned)])

PB_cor <- data.frame(matrix(NA, nrow = 730, ncol = 2))
colnames(PB_cor) <- c("AIC", "BIC")

data.xts <- xts(ORCL$Ratios.PB$NivCleaned[ORCL$NivClean_Ind], order.by = ORCL$NivClean_Ind)
colnames(data.xts) <- c("PB")
data.xts$P <- ORCL$P_xts$d.P_MA[ORCL$NivClean_Ind]

#find the best fitting model
for(i in c(1:730)){
  model <- lm(data.xts$P ~ lag(data.xts$PB, i))
  PB_cor$AIC[i] <- AIC(model)
  PB_cor$BIC[i] <- BIC(model)
}

(best_i <- order(PB_cor$AIC)[1])
(order(PB_cor$BIC)[1])

par(mfrow = c(1, 1))
plot(PB_cor$AIC, type = c("l"), main = c("AIC and BIC"))
lines(PB_cor$BIC, col = c("orange"))
abline(v = best_i)

#fitting of the best the model with the lowest AIC/BIC
data.xts$PB <- lag(ORCL$Ratios.PB$NivCleaned[ORCL$NivClean_Ind], 65)
model <- lm(data.xts$P ~ data.xts$PB, na.action = na.exclude)
summary(model)

AIC(model)
BIC(model)

plot(ts(ORCL$P_xts$d.P_MA[ORCL$NivClean_Ind]), main = c("Fitted Values"))
lines(c(rep(NA, 65), ts(model$fitted.values)), col = c("red"))
```
  
##### Betrachtung der Residuen
  
```{r }
plot.dens(as.numeric(model$residuals), title = c("Density of Residuals"), plot.norm = TRUE, plot.lines = FALSE)

par(mfrow = c(1, 2))
Acf(as.numeric(model$residuals[!is.na(model$residuals)]), main = c("ACF: Residuals"))
Pacf(as.numeric(model$residuals[!is.na(model$residuals)]), main = c("PACF: Residuals"))
par(mfrow = c(1, 1))

(model.autoarima <- auto.arima(ts(model$residuals[!is.na(model$residuals)])))

(model.stationary <- auto.arima(ts(model$residuals[!is.na(model$residuals)]), d = 0))

```
  
#### ADL
##### Berechnung:
  
```{r , fig.width= 9, fig.height=6}

ORCL$NivClean_Ind <- index(ORCL$Ratios.PB$NivCleaned[!is.na(ORCL$Ratios.PB$NivCleaned)])

AIC_BIC <- data.frame(matrix(NA, nrow = 730, ncol = 2))
colnames(AIC_BIC) <- c("AIC", "BIC")

data.xts <- xts(ORCL$Ratios.PB$NivCleaned[ORCL$NivClean_Ind], order.by = ORCL$NivClean_Ind)
colnames(data.xts) <- c("PB")
data.xts$P <- ORCL$P_xts$d.P_MA[ORCL$NivClean_Ind]

#Test for the stationary requirement of P and PB
par(mfrow = c(2, 2))
acf(data.xts$P, main = c("ACF: P"))
pacf(data.xts$P, main = c("PACF: P"))
acf(data.xts$PB, main = c("ACF: PB"))
pacf(data.xts$PB, main = c("PACF: PB"))

adf.test(data.xts$P)
adf.test(data.xts$PB)


#Fitting of the Model
model <- lm(data.xts$P ~ data.xts$PB + lag(data.xts$PB, 1) + lag(data.xts$PB, 2) + lag(data.xts$P, 1) + lag(data.xts$P, 2))
summary(model)
AIC(model)
BIC(model)

par(mfrow = c(1, 1))
plot.data <- data.xts$P
colnames(plot.data) <- c("P")
plot.data$fit <- model$fitted.values
plot(plot.data["1990/2000"], type = c("l"), col = c("black", "orange", "blue"), main = c("Fitted Values"))
plot(plot.data["2000/2010"], type = c("l"), col = c("black", "orange", "blue"), main = c("Fitted Values"))
plot(plot.data["2010/2019"], type = c("l"), col = c("black", "orange", "blue"), main = c("Fitted Values"))
```
  
##### Betrachtung der Residuen:
  
```{r }
plot.dens(as.numeric(model$residuals), title = c("Density of Residuals"), plot.norm = TRUE, plot.lines = FALSE)
par(mfrow = c(1, 2))
acf(model$residuals, main = c("ACF: Residuen"))
pacf(model$residuals, main = c("PACF: Residuen"))
par(mfrow = c(1, 1))
```

